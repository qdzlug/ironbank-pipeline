upload to s3:
  extends: .publish
  image: ${GITLAB_INTERNAL_REGISTRY}/dsop/ironbank-pipeline/python:pyyaml
  variables:
    #TODO: Put these in globals
    IMAGE_FILE: ${CI_PROJECT_NAME}-${IMG_VERSION}
    SCAN_DIRECTORY: ${ARTIFACT_STORAGE}/scan-results/
    DOCUMENTATION_DIRECTORY: ${ARTIFACT_STORAGE}/documentation/
    BASE_BUCKET_DIRECTORY: "testing/container-scan-reports"
  dependencies:
    - sign image
    - sign manifest
    - write json documentation
    - create mapping website
    - anchore scan
    - twistlock scan
    - openscap cve
    - openscap compliance
    - load scripts
  before_script:
    - pip install boto3
    - |
      if [ "${CI_COMMIT_BRANCH}" = "master" ]; then
        export BASE_BUCKET_DIRECTORY="container-scan-reports"
      fi
    - export REMOTE_REPORT_DIRECTORY="$(date +%FT%T)_${CI_COMMIT_SHA}/reports"
      #- export REMOTE_DOCUMENTATION_DIRECTORY="$(date +%FT%T)_${CI_COMMIT_SHA}"
    - export IMAGE_PATH=$(echo ${CI_PROJECT_PATH} | sed -e 's/.*dsop\/\(.*\)/\1/')
  script:
    - |
      report_name="repo_map.json"
      python3 "${PIPELINE_REPO_DIR}/stages/publish/s3_upload.py" --file ${ARTIFACT_STORAGE}/documentation/repo_map.json --bucket ${S3_REPORT_BUCKET} --dest ${BASE_BUCKET_DIRECTORY}/${IMAGE_PATH}/$report_name
    - |
      for file in $(find ${DOCUMENTATION_DIRECTORY}/reports -name "*" -type f); do
        object_path=$(echo ${file#"$ARTIFACT_STORAGE/documentation/"})
        python3 "${PIPELINE_REPO_DIR}/stages/publish/s3_upload.py" --file $file --bucket ${S3_REPORT_BUCKET} --dest ${BASE_BUCKET_DIRECTORY}/${IMAGE_PATH}/${IMG_VERSION}/${REMOTE_REPORT_DIRECTORY}/$object_path ;
      done
    - |
      for file in $(find ${SCAN_DIRECTORY} -name "*" -type f); do
        report_name=$(echo $file | rev | cut -d/ -f1-2 | rev)
        python3 "${PIPELINE_REPO_DIR}/stages/publish/s3_upload.py" --file $file --bucket ${S3_REPORT_BUCKET} --dest ${BASE_BUCKET_DIRECTORY}/${IMAGE_PATH}/${IMG_VERSION}/${REMOTE_REPORT_DIRECTORY}/$report_name ;
      done
      #NOTE: keeping this around until we determine if we need to keep the signature generation in the upload to s3 via s3_upload.py
      #    - |
      #      FILES=${SCAN_DIRECTORY}/*
      #      AWS_KEY="${S3_ACCESS_KEY}"
      #      AWS_SECRET="${S3_SECRET_KEY}"
      #      S3_BUCKET="${S3_REPORT_BUCKET}"
      #      S3_BUCKET_PATH="/${CI_PROJECT_NAME}-${IMG_VERSION}/${REPORT_DIRECTORY}"

      #      bucket=${S3_BUCKET}
      #      bucket_path=${S3_BUCKET_PATH}
      #      date=$(date +"%a, %d %b %Y %T %z")
      #      acl="x-amz-acl:private"
      #      content_type="application/octet-stream"
      #      sig_string="PUT\n\n$content_type\n$date\n$acl\n/$bucket$bucket_path$file"
      #      signature=$(echo -en "${sig_string}" | openssl sha1 -hmac "${AWS_SECRET}" -binary | base64)

      #      curl -X PUT -T "$bucket_path/$f" \
      #      -H "Host: $bucket.s3.amazonaws.com" \
      #      -H "Date: $date" \
      #      -H "Content-Type: $content_type" \
      #      -H "$acl" \
      #      -H "Authorization: AWS ${AWS_KEY}:$signature" \
      #      "https://$bucket.s3.amazonaws.com$bucket_path$file"
