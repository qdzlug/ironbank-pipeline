upload to s3:
  extends: .publish
  image: ${GITLAB_INTERNAL_REGISTRY}/ironbank-tools/ironbank-pipeline/python:pyyaml
  resource_group: s3_phase
  # only:
  #   - master
  #   - development
  variables:
    #TODO: Put these in globals
    IMAGE_FILE: ${CI_PROJECT_NAME}-${IMG_VERSION}
    SCAN_DIRECTORY: ${ARTIFACT_STORAGE}/scan-results
    DOCUMENTATION_DIRECTORY: ${ARTIFACT_STORAGE}/documentation
    BUILD_DIRECTORY: ${ARTIFACT_STORAGE}/build
    SIG_FILE: signature
    BASE_BUCKET_DIRECTORY: "testing/container-scan-reports"
    DOCUMENTATION_FILENAME: documentation
    ARTIFACT_DIR: ${ARTIFACT_STORAGE}/documentation
    REPORT_TAR_NAME: ${CI_PROJECT_NAME}-${IMG_VERSION}-reports-signature.tar.gz
  dependencies:
    - build
    - sign image
    - sign manifest
    - write json documentation
    - anchore scan
    - twistlock scan
    - openscap cve
    - openscap compliance
    - csv output
    - load scripts
    - wl compare lint
  before_script:
    - mkdir -p ${ARTIFACT_DIR}
    - pip install boto3 ushlex
    - |
      if [ "${CI_COMMIT_BRANCH}" == "master" ]; then
        export BASE_BUCKET_DIRECTORY="container-scan-reports"
      fi
    - |
      export REMOTE_REPORT_DIRECTORY="${CI_PIPELINE_ID}/reports"
      export REMOTE_DOCUMENTATION_DIRECTORY="${CI_PIPELINE_ID}"
      export IMAGE_PATH=$(echo ${CI_PROJECT_PATH} | sed -e 's/.*dsop\/\(.*\)/\1/')
      export S3_HTML_LINK="https://s3-us-gov-west-1.amazonaws.com/${S3_REPORT_BUCKET}/${BASE_BUCKET_DIRECTORY}/${IMAGE_PATH}/${IMG_VERSION}"
      export GPG_PUB_KEY=$(awk '{printf "%s\\n", $0}' ${IB_CONTAINER_GPG_PUBKEY})
      export PROJECT_README=$(find . -name "README*" -type f -maxdepth 1 | rev | cut -d/ -f1 | rev)
      export PROJECT_LICENSE=$(find . -name "LICENSE*" -type f -maxdepth 1 | rev | cut -d/ -f1 | rev)
    - python3 ${PIPELINE_REPO_DIR}/stages/publish/create_repo_map.py --target ${BASE_BUCKET_DIRECTORY}/${IMAGE_PATH}/repo_map.json
    - mkdir reports
    - cp -r ${DOCUMENTATION_DIRECTORY}/reports/* reports/
    - cp -r ${SCAN_DIRECTORY}/* reports/
    - cp ${BUILD_DIRECTORY}/${CI_PROJECT_NAME}-${IMG_VERSION}.tar reports/${CI_PROJECT_NAME}-${IMG_VERSION}.tar
    - cp "${PROJECT_LICENSE}" "${PROJECT_README}" reports/
    - ls reports
    - tar -zcvf "${REPORT_TAR_NAME}" reports
  script:
    - |
      python3 "${PIPELINE_REPO_DIR}/stages/publish/s3_upload.py" --file repo_map.json --bucket "${S3_REPORT_BUCKET}" --dest "${BASE_BUCKET_DIRECTORY}/${IM_NAME}/repo_map.json"
    - |
      for file in $(find "${DOCUMENTATION_DIRECTORY}" -name "*" -type f); do
        object_path=$(echo ${file#"$ARTIFACT_STORAGE/documentation/"})
        python3 "${PIPELINE_REPO_DIR}/stages/publish/s3_upload.py" --file $file --bucket "${S3_REPORT_BUCKET}" --dest "${BASE_BUCKET_DIRECTORY}/${IMAGE_PATH}/${IMG_VERSION}/${REMOTE_DOCUMENTATION_DIRECTORY}/$object_path" ;
      done
    - |
      for file in $(find "${SCAN_DIRECTORY}" -name "*" -type f); do
        report_name=$(echo $file | rev | cut -d/ -f1-2 | rev)
        echo $file
        python3 "${PIPELINE_REPO_DIR}/stages/publish/s3_upload.py" --file $file --bucket "${S3_REPORT_BUCKET}" --dest "${BASE_BUCKET_DIRECTORY}/${IMAGE_PATH}/${IMG_VERSION}/${REMOTE_REPORT_DIRECTORY}/$report_name" ;
      done
    - python3 "${PIPELINE_REPO_DIR}/stages/publish/s3_upload.py" --file "${PROJECT_README}" --bucket "${S3_REPORT_BUCKET}" --dest "${BASE_BUCKET_DIRECTORY}/${IMAGE_PATH}/${IMG_VERSION}/${REMOTE_REPORT_DIRECTORY}/${PROJECT_README}"
    - python3 "${PIPELINE_REPO_DIR}/stages/publish/s3_upload.py" --file "${PROJECT_LICENSE}" --bucket "${S3_REPORT_BUCKET}" --dest "${BASE_BUCKET_DIRECTORY}/${IMAGE_PATH}/${IMG_VERSION}/${REMOTE_REPORT_DIRECTORY}/${PROJECT_LICENSE}"
    - python3 "${PIPELINE_REPO_DIR}/stages/publish/s3_upload.py" --file "${REPORT_TAR_NAME}" --bucket "${S3_REPORT_BUCKET}" --dest "${BASE_BUCKET_DIRECTORY}/${IMAGE_PATH}/${IMG_VERSION}/${REMOTE_REPORT_DIRECTORY}/${REPORT_TAR_NAME}"
